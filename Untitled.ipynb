{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from math import floor\n",
    "import numpy as np\n",
    "import time\n",
    "from functools import partial\n",
    "from random import random\n",
    "import os\n",
    "import load_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_size = 64\n",
    "latent_size = 64\n",
    "BATCH_SIZE = 32\n",
    "directory = \"Faces256\"\n",
    "suff = 'jpg'\n",
    "cmode = 'RGB'\n",
    "channels = 3\n",
    "size_adjusted = False\n",
    "\n",
    "k_images = 3\n",
    "\n",
    "cha = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(n):\n",
    "    return np.random.normal(0.0, 1.0, size = [n, latent_size])\n",
    "\n",
    "\n",
    "class dataGenerator(object):\n",
    "\n",
    "    def __init__(self, loc, flip = True, suffix = 'png'):\n",
    "        self.flip = flip\n",
    "        self.suffix = suffix\n",
    "        self.files = []\n",
    "        self.n = 1e10\n",
    "\n",
    "        print(\"Importing Images...\")\n",
    "\n",
    "        try:\n",
    "            os.mkdir(\"../data/\" + loc + \"-npy-\" + str(im_size))\n",
    "        except:\n",
    "            self.load_from_npy(loc)\n",
    "            return\n",
    "\n",
    "        for dirpath, dirnames, filenames in os.walk(\"data/\" + loc):\n",
    "            for filename in [f for f in filenames if f.endswith(\".\"+str(self.suffix))]:\n",
    "                print('\\r' + str(len(self.files)), end = '\\r')\n",
    "                fname = os.path.join(dirpath, filename)\n",
    "                temp = Image.open(fname).convert(cmode)\n",
    "                if not size_adjusted:\n",
    "                    temp = temp.resize((im_size, im_size), Image.BILINEAR)\n",
    "                temp = np.array(temp, dtype='uint8')\n",
    "                self.files.append(temp)\n",
    "                if self.flip:\n",
    "                    self.files.append(np.flip(temp, 1))\n",
    "\n",
    "        self.files = np.array(self.files)\n",
    "        np.save(\"data/\" + loc + \"-npy-\" + str(im_size) + \"/data.npy\", self.files)\n",
    "\n",
    "        self.n = self.files.shape[0]\n",
    "\n",
    "        print(\"Found \" + str(self.n) + \" images in \" + loc + \".\")\n",
    "\n",
    "    def load_from_npy(self, loc):\n",
    "\n",
    "        print(\"Loading from .npy files.\")\n",
    "\n",
    "        self.files = np.load(\"data/\" + str(loc) + \"-npy-\" + str(im_size) + \"/data.npy\")\n",
    "\n",
    "        self.n = self.files.shape[0]\n",
    "\n",
    "\n",
    "    def get_batch(self, num):\n",
    "\n",
    "        idx = np.random.randint(0, self.n - 200, num)\n",
    "        out = []\n",
    "\n",
    "        for i in range(num):\n",
    "            out.append(self.files[idx[i]])\n",
    "\n",
    "        return np.array(out).astype('float32') / 255.0\n",
    "\n",
    "    def get_test_batch(self, num):\n",
    "\n",
    "        idx = np.random.randint(self.n - 200, self.n, num)\n",
    "        out = []\n",
    "\n",
    "        for i in range(num):\n",
    "            out.append(self.files[idx[i]])\n",
    "\n",
    "        return np.array(out).astype('float32') / 255.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print iterations progress\n",
    "def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 50, fill = 'â–ˆ'):\n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "    \"\"\"\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print('\\r %s |%s| %s%% %s' % (prefix, bar, percent, suffix), end = '\\r')\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total:\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, Dense, AveragePooling2D, Activation, Cropping2D, Dropout, BatchNormalization\n",
    "from keras.layers import Reshape, UpSampling2D, Flatten, Input, add, Lambda, concatenate, LeakyReLU, multiply\n",
    "from keras.layers import GlobalAveragePooling2D, average\n",
    "from keras.models import model_from_json, Model\n",
    "from keras.initializers import VarianceScaling\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "def gradient_penalty_loss(y_true, y_pred, averaged_samples, weight, sample_weight=None):\n",
    "    gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "    gradients_sqr = K.square(gradients)\n",
    "    gradient_penalty = K.sum(gradients_sqr,\n",
    "                              axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "\n",
    "    # (weight / 2) * ||grad||^2\n",
    "    # Penalize the gradient norm\n",
    "    return K.mean(gradient_penalty) * (weight / 2)\n",
    "\n",
    "def hinge_d(y_true, y_pred):\n",
    "    return K.mean(K.relu(1.0 - (y_true * y_pred)))\n",
    "\n",
    "def w_loss(y_true, y_pred):\n",
    "    return K.mean(y_true * y_pred)\n",
    "\n",
    "def g_block(inp, fil, u = True):\n",
    "\n",
    "    if u:\n",
    "        out = UpSampling2D(interpolation = 'bilinear')(inp)\n",
    "    else:\n",
    "        out = Activation('linear')(inp)\n",
    "\n",
    "    skip = Conv2D(fil, 1, padding = 'same', kernel_initializer = 'he_normal')(out)\n",
    "\n",
    "    out = Conv2D(filters = fil, kernel_size = 3, padding = 'same', kernel_initializer = 'he_normal')(out)\n",
    "    out = LeakyReLU(0.2)(out)\n",
    "\n",
    "    out = Conv2D(filters = fil, kernel_size = 3, padding = 'same', kernel_initializer = 'he_normal')(out)\n",
    "    out = LeakyReLU(0.2)(out)\n",
    "\n",
    "    out = Conv2D(fil, 1, padding = 'same', kernel_initializer = 'he_normal')(out)\n",
    "\n",
    "    out = add([out, skip])\n",
    "    out = LeakyReLU(0.2)(out)\n",
    "\n",
    "    return out\n",
    "\n",
    "def d_block(inp, fil, p = True):\n",
    "\n",
    "    skip = Conv2D(fil, 1, padding = 'same', kernel_initializer = 'he_normal')(inp)\n",
    "\n",
    "    out = Conv2D(filters = fil, kernel_size = 3, padding = 'same', kernel_initializer = 'he_normal')(inp)\n",
    "    out = LeakyReLU(0.2)(out)\n",
    "\n",
    "    out = Conv2D(filters = fil, kernel_size = 3, padding = 'same', kernel_initializer = 'he_normal')(out)\n",
    "    out = LeakyReLU(0.2)(out)\n",
    "\n",
    "    out = Conv2D(fil, 1, padding = 'same', kernel_initializer = 'he_normal')(out)\n",
    "\n",
    "    out = add([out, skip])\n",
    "    out = LeakyReLU(0.2)(out)\n",
    "\n",
    "    if p:\n",
    "        out = AveragePooling2D()(out)\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(object):\n",
    "\n",
    "    def __init__(self, steps = 1, lr = 0.0001, decay = 0.00001):\n",
    "\n",
    "        #Models\n",
    "        self.D = None\n",
    "        self.E = None\n",
    "        self.G = None\n",
    "\n",
    "        self.GE = None\n",
    "        self.EE = None\n",
    "\n",
    "        self.DM = None\n",
    "        self.AM = None\n",
    "\n",
    "        #Config\n",
    "        self.LR = lr\n",
    "        self.steps = steps\n",
    "        self.beta = 0.999\n",
    "\n",
    "        #Init Models\n",
    "        self.discriminator()\n",
    "        self.generator()\n",
    "        self.encoder()\n",
    "\n",
    "        self.EE = model_from_json(self.E.to_json())\n",
    "        self.EE.set_weights(self.E.get_weights())\n",
    "\n",
    "        self.GE = model_from_json(self.G.to_json())\n",
    "        self.GE.set_weights(self.G.get_weights())\n",
    "\n",
    "    def discriminator(self):\n",
    "\n",
    "        if self.D:\n",
    "            return self.D\n",
    "\n",
    "        inp = Input(shape = [im_size, im_size, 3])\n",
    "        inpl = Input(shape = [latent_size])\n",
    "\n",
    "        #Latent input\n",
    "        l = Dense(512, kernel_initializer = 'he_normal')(inpl)\n",
    "        l = LeakyReLU(0.2)(l)\n",
    "        l = Dense(512, kernel_initializer = 'he_normal')(l)\n",
    "        l = LeakyReLU(0.2)(l)\n",
    "        l = Dense(512, kernel_initializer = 'he_normal')(l)\n",
    "        l = LeakyReLU(0.2)(l)\n",
    "\n",
    "        x = d_block(inp, 1 * cha)   #64\n",
    "        x = d_block(x, 2 * cha)   #32\n",
    "        x = d_block(x, 3 * cha)   #16\n",
    "        x = d_block(x, 4 * cha)  #8\n",
    "        x = d_block(x, 8 * cha)  #4\n",
    "        x = d_block(x, 16 * cha, p = False)  #4\n",
    "\n",
    "        x = Flatten()(x)\n",
    "\n",
    "        x = concatenate([x, l])\n",
    "\n",
    "        x = Dense(16 * cha, kernel_initializer = 'he_normal')(x)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "\n",
    "        x = Dense(1, kernel_initializer = 'he_normal')(x)\n",
    "\n",
    "        self.D = Model(inputs = [inp, inpl], outputs = x)\n",
    "\n",
    "        return self.D\n",
    "\n",
    "    def generator(self):\n",
    "\n",
    "        if self.G:\n",
    "            return self.G\n",
    "\n",
    "        #Inputs\n",
    "        inp = Input(shape = [latent_size])\n",
    "\n",
    "        #Latent\n",
    "\n",
    "        #Actual Model\n",
    "        x = Dense(4*4*16*cha, kernel_initializer = 'he_normal')(inp)\n",
    "        x = Reshape([4, 4, 16*cha])(x)\n",
    "\n",
    "        x = g_block(x, 16 * cha, u = False)  #4\n",
    "        x = g_block(x, 8 * cha)  #8\n",
    "        x = g_block(x, 4 * cha)  #16\n",
    "        x = g_block(x, 3 * cha)   #32\n",
    "        x = g_block(x, 2 * cha)   #64\n",
    "        x = g_block(x, 1 * cha)   #128\n",
    "\n",
    "        x = Conv2D(filters = 3, kernel_size = 1, activation = 'sigmoid', padding = 'same', kernel_initializer = 'he_normal')(x)\n",
    "\n",
    "        self.G = Model(inputs = inp, outputs = x)\n",
    "\n",
    "        return self.G\n",
    "\n",
    "    def encoder(self):\n",
    "\n",
    "        if self.E:\n",
    "            return self.E\n",
    "\n",
    "        inp = Input(shape = [im_size, im_size, 3])\n",
    "\n",
    "        x = d_block(inp, 1 * cha)   #64\n",
    "        x = d_block(x, 2 * cha)   #32\n",
    "        x = d_block(x, 3 * cha)   #16\n",
    "        x = d_block(x, 4 * cha)  #8\n",
    "        x = d_block(x, 8 * cha)  #4\n",
    "        x = d_block(x, 16 * cha, p = False)  #4\n",
    "\n",
    "        x = Flatten()(x)\n",
    "\n",
    "        x = Dense(16 * cha, kernel_initializer = 'he_normal')(x)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "\n",
    "        x = Dense(latent_size, kernel_initializer = 'he_normal', bias_initializer = 'zeros')(x)\n",
    "\n",
    "        self.E = Model(inputs = inp, outputs = x)\n",
    "\n",
    "        return self.E\n",
    "\n",
    "    def AdModel(self):\n",
    "\n",
    "        #D does not update\n",
    "        self.D.trainable = False\n",
    "        for layer in self.D.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        #G does update\n",
    "        self.G.trainable = True\n",
    "        for layer in self.G.layers:\n",
    "            layer.trainable = True\n",
    "\n",
    "        #E does update\n",
    "        self.E.trainable = True\n",
    "        for layer in self.E.layers:\n",
    "            layer.trainable = True\n",
    "\n",
    "        # Fake Latent / Real Image\n",
    "        ri = Input(shape = [im_size, im_size, 3])\n",
    "\n",
    "        er = self.E(ri)\n",
    "        dr = self.D([ri, er])\n",
    "\n",
    "        # Real Latent / Fake Image\n",
    "        gi = Input(shape = [latent_size])\n",
    "\n",
    "        gf = self.G(gi)\n",
    "        df = self.D([gf, gi])\n",
    "\n",
    "        self.AM = Model(inputs = [ri, gi], outputs = [dr, df])\n",
    "\n",
    "        self.AM.compile(optimizer = Adam(self.LR, beta_1 = 0, beta_2 = 0.099), loss = [w_loss, w_loss])\n",
    "\n",
    "        return self.AM\n",
    "\n",
    "    def DisModel(self):\n",
    "\n",
    "        #D does update\n",
    "        self.D.trainable = True\n",
    "        for layer in self.D.layers:\n",
    "            layer.trainable = True\n",
    "\n",
    "        #G does not update\n",
    "        self.G.trainable = False\n",
    "        for layer in self.G.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        #E does update\n",
    "        self.E.trainable = False\n",
    "        for layer in self.E.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        # Fake Latent / Real Image\n",
    "        ri = Input(shape = [im_size, im_size, 3])\n",
    "\n",
    "        er = self.E(ri)\n",
    "        dr = self.D([ri, er])\n",
    "\n",
    "        # Real Latent / Fake Image\n",
    "        gi = Input(shape = [latent_size])\n",
    "\n",
    "        gf = self.G(gi)\n",
    "        df = self.D([gf, gi])\n",
    "\n",
    "        self.DM = Model(inputs = [ri, gi], outputs = [dr, df, df])\n",
    "\n",
    "        # Create partial of gradient penalty loss\n",
    "        # For r1, averaged_samples = ri\n",
    "        # For r2, averaged_samples = gf\n",
    "        # Weight of 10 typically works\n",
    "        partial_gp_loss = partial(gradient_penalty_loss, averaged_samples = [gf, gi], weight = 5)\n",
    "\n",
    "        #Compile With Corresponding Loss Functions\n",
    "        self.DM.compile(optimizer = Adam(self.LR, beta_1 = 0, beta_2 = 0.909), loss=[hinge_d, hinge_d, partial_gp_loss])\n",
    "\n",
    "        return self.DM\n",
    "\n",
    "    def EMA(self):\n",
    "\n",
    "        start = time.clock()\n",
    "\n",
    "        for i in range(len(self.G.layers)):\n",
    "            up_weight = self.G.layers[i].get_weights()\n",
    "            old_weight = self.GE.layers[i].get_weights()\n",
    "            new_weight = []\n",
    "            for j in range(len(up_weight)):\n",
    "                new_weight.append(old_weight[j] * self.beta + (1-self.beta) * up_weight[j])\n",
    "            self.GE.layers[i].set_weights(new_weight)\n",
    "\n",
    "        for i in range(len(self.E.layers)):\n",
    "            up_weight = self.E.layers[i].get_weights()\n",
    "            old_weight = self.EE.layers[i].get_weights()\n",
    "            new_weight = []\n",
    "            for j in range(len(up_weight)):\n",
    "                new_weight.append(old_weight[j] * self.beta + (1-self.beta) * up_weight[j])\n",
    "            self.EE.layers[i].set_weights(new_weight)\n",
    "\n",
    "        #print(\"Moved Average. \" + str(time.clock() - start) + \"s\")\n",
    "\n",
    "    def MAinit(self):\n",
    "        self.EE.set_weights(self.E.get_weights())\n",
    "        self.GE.set_weights(self.G.get_weights())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Images...\n",
      "Loading from .npy files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sofiapaganin/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/Faces256-npy-64/data.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-34e95def3a85>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loc, flip, suffix)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"-npy-\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/Faces256-npy-64'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-67b21d16a85c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBiGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-67b21d16a85c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, steps, lr, decay, silent)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise_level\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msilent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-34e95def3a85>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loc, flip, suffix)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"-npy-\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_npy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-34e95def3a85>\u001b[0m in \u001b[0;36mload_from_npy\u001b[0;34m(self, loc)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading from .npy files.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"-npy-\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/data.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/Faces256-npy-64/data.npy'"
     ]
    }
   ],
   "source": [
    "class BiGAN(object):\n",
    "\n",
    "    def __init__(self, steps = 1, lr = 0.0001, decay = 0.00001, silent = True):\n",
    "\n",
    "        self.GAN = GAN(steps = steps, lr = lr, decay = decay)\n",
    "        self.DisModel = self.GAN.DisModel()\n",
    "        self.AdModel = self.GAN.AdModel()\n",
    "\n",
    "        self.lastblip = time.clock()\n",
    "\n",
    "        self.noise_level = 0\n",
    "\n",
    "        self.im = dataGenerator(directory, suffix = suff, flip = True)\n",
    "\n",
    "        self.silent = silent\n",
    "\n",
    "        #Train Generator to be in the middle, not all the way at real. Apparently works better??\n",
    "        self.ones = np.ones((BATCH_SIZE, 1), dtype=np.float32)\n",
    "        self.zeros = np.zeros((BATCH_SIZE, 1), dtype=np.float32)\n",
    "        self.nones = -self.ones\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        #Train Alternating\n",
    "        a = self.train_dis()\n",
    "        b = self.train_gen()\n",
    "\n",
    "        if self.GAN.steps % 10 == 0:\n",
    "            self.GAN.EMA()\n",
    "\n",
    "        if self.GAN.steps == 20000:\n",
    "            self.GAN.MAinit()\n",
    "\n",
    "\n",
    "        #Print info\n",
    "        if self.GAN.steps % 100 == 0 and not self.silent:\n",
    "            print(\"\\n\\nRound \" + str(self.GAN.steps) + \":\")\n",
    "            print(\"D: \" + str(a))\n",
    "            print(\"G: \" + str(b))\n",
    "            s = round((time.clock() - self.lastblip), 4)\n",
    "            steps_per_second = 100 / s\n",
    "            steps_per_minute = steps_per_second * 60\n",
    "            steps_per_hour = steps_per_minute * 60\n",
    "            print(\"Steps/Second: \" + str(round(steps_per_second, 2)))\n",
    "            print(\"Steps/Hour: \" + str(round(steps_per_hour)))\n",
    "            min1k = floor(1000/steps_per_minute)\n",
    "            sec1k = floor(1000/steps_per_second) % 60\n",
    "            print(\"1k Steps: \" + str(min1k) + \":\" + str(sec1k))\n",
    "            self.lastblip = time.clock()\n",
    "            steps_left = 200000 - self.GAN.steps + 1e-7\n",
    "            hours_left = steps_left // steps_per_hour\n",
    "            minutes_left = (steps_left // steps_per_minute) % 60\n",
    "\n",
    "            print(\"Til Completion: \" + str(int(hours_left)) + \"h\" + str(int(minutes_left)) + \"m\")\n",
    "            print()\n",
    "\n",
    "            #Save Model\n",
    "            if self.GAN.steps % 500 == 0:\n",
    "                self.save(floor(self.GAN.steps / 10000))\n",
    "            if self.GAN.steps % 1000 == 0 or (self.GAN.steps % 100 == 0 and self.GAN.steps < 1000):\n",
    "                self.evaluate(floor(self.GAN.steps / 1000))\n",
    "\n",
    "\n",
    "        printProgressBar(self.GAN.steps % 100, 99, decimals = 0)\n",
    "\n",
    "        self.GAN.steps = self.GAN.steps + 1\n",
    "\n",
    "    def train_dis(self):\n",
    "\n",
    "        #Get Data\n",
    "        train_data = [self.im.get_batch(BATCH_SIZE), noise(BATCH_SIZE)]\n",
    "\n",
    "        #Train\n",
    "        d_loss = self.DisModel.train_on_batch(train_data, [self.ones, self.nones, self.ones])\n",
    "\n",
    "        return d_loss\n",
    "\n",
    "    def train_gen(self):\n",
    "\n",
    "        #Train\n",
    "        train_data = [self.im.get_batch(BATCH_SIZE), noise(BATCH_SIZE)]\n",
    "\n",
    "        g_loss = self.AdModel.train_on_batch(train_data, [self.ones, self.nones])\n",
    "\n",
    "        return g_loss\n",
    "\n",
    "    def evaluate(self, num = 0):\n",
    "\n",
    "        n1 = noise(32)\n",
    "\n",
    "        generated_images = self.GAN.G.predict(n1, batch_size = BATCH_SIZE)\n",
    "\n",
    "        real_images = self.im.get_test_batch(16)\n",
    "        latent_codes = self.GAN.E.predict(real_images, batch_size = BATCH_SIZE)\n",
    "        reconstructed_images = self.GAN.G.predict(latent_codes, batch_size = BATCH_SIZE)\n",
    "\n",
    "        print(\"E Mean: \" + str(np.mean(latent_codes)))\n",
    "        print(\"E Std: \" + str(np.std(latent_codes)))\n",
    "        print(\"E Std Featurewise: \" + str(np.mean(np.std(latent_codes, axis = 0))))\n",
    "        print()\n",
    "\n",
    "        r = []\n",
    "\n",
    "        for i in range(0, 32, 8):\n",
    "            r.append(np.concatenate(generated_images[i:i+8], axis = 1))\n",
    "\n",
    "        hline = np.zeros([16, 8 * im_size, 3])\n",
    "        r.append(hline)\n",
    "\n",
    "        for i in range(0, 16, 8):\n",
    "            r.append(np.concatenate(real_images[i:i+8], axis = 1))\n",
    "            r.append(np.concatenate(reconstructed_images[i:i+8], axis = 1))\n",
    "\n",
    "        c1 = np.concatenate(r, axis = 0)\n",
    "\n",
    "        x = Image.fromarray(np.uint8(c1*255))\n",
    "\n",
    "        x.save(\"Results/i\"+str(num)+\".png\")\n",
    "\n",
    "        # Moving Average\n",
    "\n",
    "        n1 = noise(32)\n",
    "\n",
    "        generated_images = self.GAN.GE.predict(n1, batch_size = BATCH_SIZE)\n",
    "\n",
    "        latent_codes = self.GAN.EE.predict(real_images, batch_size = BATCH_SIZE)\n",
    "        reconstructed_images = self.GAN.GE.predict(latent_codes, batch_size = BATCH_SIZE)\n",
    "\n",
    "        r = []\n",
    "\n",
    "        for i in range(0, 32, 8):\n",
    "            r.append(np.concatenate(generated_images[i:i+8], axis = 1))\n",
    "\n",
    "        hline = np.zeros([16, 8 * im_size, 3])\n",
    "        r.append(hline)\n",
    "\n",
    "        for i in range(0, 16, 8):\n",
    "            r.append(np.concatenate(real_images[i:i+8], axis = 1))\n",
    "            r.append(np.concatenate(reconstructed_images[i:i+8], axis = 1))\n",
    "\n",
    "        c1 = np.concatenate(r, axis = 0)\n",
    "\n",
    "        x = Image.fromarray(np.uint8(c1*255))\n",
    "\n",
    "        x.save(\"Results/i\"+str(num)+\"-ema.png\")\n",
    "\n",
    "\n",
    "    def prepareSamples(self, cnum = 0, num = 1000): #8x8 images, bottom row is constant\n",
    "\n",
    "        try:\n",
    "            os.mkdir(\"Results/Samples-c\" + str(cnum))\n",
    "        except:\n",
    "            x = 0\n",
    "\n",
    "        im = self.im.get_class(cnum)\n",
    "        e = self.GAN.E.predict(im, batch_size = BATCH_SIZE * k_images)\n",
    "\n",
    "        mean = np.mean(e, axis = 0)\n",
    "        std = np.std(e, axis = 0)\n",
    "\n",
    "        n = noise(num)\n",
    "        nc = nClass(num, mean, std)\n",
    "\n",
    "        im = self.GAN.G.predict([n, nc], batch_size = BATCH_SIZE)\n",
    "\n",
    "        for i in range(im.shape[0]):\n",
    "\n",
    "            x = Image.fromarray(np.uint8(im[i]*255), mode = 'RGB')\n",
    "\n",
    "            x.save(\"Results/Samples-c\" + str(cnum) + \"/im (\"+str(i+1)+\").png\")\n",
    "\n",
    "    def saveModel(self, model, name, num):\n",
    "        json = model.to_json()\n",
    "        with open(\"Models/\"+name+\".json\", \"w\") as json_file:\n",
    "            json_file.write(json)\n",
    "\n",
    "        model.save_weights(\"Models/\"+name+\"_\"+str(num)+\".h5\")\n",
    "\n",
    "    def loadModel(self, name, num):\n",
    "\n",
    "        file = open(\"Models/\"+name+\".json\", 'r')\n",
    "        json = file.read()\n",
    "        file.close()\n",
    "\n",
    "        mod = model_from_json(json)\n",
    "        mod.load_weights(\"Models/\"+name+\"_\"+str(num)+\".h5\")\n",
    "\n",
    "        return mod\n",
    "\n",
    "    def save(self, num): #Save JSON and Weights into /Models/\n",
    "        self.saveModel(self.GAN.G, \"gen\", num)\n",
    "        self.saveModel(self.GAN.D, \"dis\", num)\n",
    "        self.saveModel(self.GAN.E, \"enc\", num)\n",
    "\n",
    "        self.saveModel(self.GAN.GE, \"genMA\", num)\n",
    "        self.saveModel(self.GAN.EE, \"encMA\", num)\n",
    "\n",
    "\n",
    "    def load(self, num): #Load JSON and Weights from /Models/\n",
    "        steps1 = self.GAN.steps\n",
    "\n",
    "        #Load Models\n",
    "        self.GAN.G = self.loadModel(\"gen\", num)\n",
    "        self.GAN.D = self.loadModel(\"dis\", num)\n",
    "        self.GAN.E = self.loadModel(\"enc\", num)\n",
    "\n",
    "        self.GAN.GE = self.loadModel(\"genMA\", num)\n",
    "        self.GAN.EE = self.loadModel(\"encMA\", num)\n",
    "\n",
    "        self.GAN.steps = steps1\n",
    "\n",
    "        self.DisModel = self.GAN.DisModel()\n",
    "        self.AdModel = self.GAN.AdModel()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = BiGAN(lr = 0.0001, silent = False)\n",
    "    model.evaluate(0)\n",
    "\n",
    "    while model.GAN.steps <= 600000:\n",
    "        model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
